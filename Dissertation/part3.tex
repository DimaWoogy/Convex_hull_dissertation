\chapter{Разработка предлагаемого алгоритма и системы сравнения быстродействия алгоритмов построения выпуклых оболочек} \label{chapt3}

\section{Разработка методики сравнения алгоритмов}

\subsection{Минусы классической методики сравнения}

Большинство сравнений алгоритмов построения выпуклой оболочки не зависят от выходных данных. А как было продемонстрировано в предыдущих главах именно от количества точек в финальной выпуклой оболочке зависит время работы многих из них.

Большинство сравнений полагается на некоторые допущения по случайному распределению точек на плоскости. Вот некоторые из них \cite{chadnov2004algorithmsComparison}:

\begin{enumerate}
	\item Равномерное распределение в единичном квадрате.
	\item Равномерное распределение в единичном круге.
	\item Нормальное распределение в единичном квадрате.
	\item Распределение Лапласа в единичном квадрате с центром распределения в точке $(0.5, 0.5)$.
	\item Равномерное распределение точек на окружности.
\end{enumerate}

%TODO: добавить рисунки разных распределений

Как видно из этого списка, количество точек в финальной выпуклой оболочке будет каким-то фиксированным для каждого из выбранного способа. Например, при распределении внутри единичного круга число вершин в выпуклой оболочке для $n$ точек будет $\theta(n^{1/3})$ \cite{algolist2010convexhull}. А для распределения на окружности очевидно, что количество точек в выпуклой оболочке будет равно изначальному количеству точек.

Такой подход не даёт полноту картины для разного количества точек в выпуклой оболочке. Он привязывает к сравнению всего лишь на основе одного параметра - количества точек. Поэтому было решено разработать новую методику сравнения алгоритмов, которая бы давала эту возможность.

\subsection{Идея сравнения на основе выходного параметра алгоритма}

Основная идея, которая будет использоваться при сравнении алгоритмов - это мы будем сравнивать не только опираясь на $n$ (изначальное количество точек), но и на $h$ (количество точек в выпуклой оболочке).

Для того, чтобы достичь этого, необходимо придумать способ генерировать тестовые данные с фиксированным процентом точек, которые будут в выпуклой оболочке. Сперва генерация происходит на окружности, эти точки точно будут на выпуклой оболочке. Это показано на рисунке \ref{img:points_gen_1}. После чего необходимо сгенерировать точки, которые будут лежать внутри выпуклой оболочки и не попадут в неё. Это делается с помощью генерации точек внутри круга, что показано на рисунке \ref{img:points_gen_2}. Финальным шагом мы перемешываем эти точки и всё. Входные данные с фиксированным процентом точек на выпуклой оболочке готовы.

\begin{figure}[H]
	{\centering
		\hfill
		\subbottom[\label{img:points_gen_1}]{%
			\includesvg[width=0.45\linewidth]{gen_1}}
		\hfill
		\subbottom[\label{img:points_gen_2}]{%
			\includesvg[width=0.45\linewidth]{gen_2}}
		\hfill
	}
	\caption{Генерирование точек с фиксированным процентом на выпуклой оболочке}
	\label{img:points_gen}
\end{figure}

\section{Сравнение алгоритмов}

\subsection{Описание используемых алгоритмов и параметров сравнения}

Для сравнительного тестирования была выбрана библиотека CGAL, как одна из самых популярных библиотек для вычислительной геометрии. Эта библиотека предоставляет несколько функций для построения выпуклой оболочки \cite{cgalconvexhull}:
\begin{itemize}
	\item $ch\_akl\_toussaint$ - функция, использующая алгоритм Akl-Toussaint\cite{akl1978fast}, сложность равна $O(n \log n)$;
	\item $ch\_bykat$ - функция, использующая алгоритм Eddy\cite{eddy1977new}, сложность равна $O(nh)$;
	\item $ch\_bykat$ - функция, использующая нерекурсивную версию алгоритма Eddy\cite{bykat1978convex}, сложность равна $O(nh)$;
	\item $ch\_graham\_andrew$ - функция, использующая версию Andrew алгоритма Грэхема\cite{andrew1979another}, сложность равна $O(n \log n)$;
	\item $ch\_jarvis$ - функция, использующая алгоритм Джарвиса\cite{jarvis1973Jarvis}, сложность равна $O(nh)$.
\end{itemize}

Как видно всего 2 алгоритма имеют сложность $O(n \log h)$. Мы будем проводить сравнение с алгоритмом Грэхема как с самым популярным из предложенных.

\subsection{Результаты}

Один из способов протестировать алгоритм с помощью новой методики - это зафиксировать количество точек в изначальном множестве и изменять процент точек, которые находятся в выпуклой оболочке. Если сделать это, то получаться графики \ref{img:comparison_10000}, \ref{img:comparison_50000}, \ref{img:comparison_100000} для 10, 50 и 100 тысяч точек соответственно.

Легко видеть, что предлагаемый в этой работе алгоритм работает быстрее алгоритма Грэхема только на маленьком проценте точек. Это объясняется как раз лучшей сложностью алгоритма. Алгоритм Грэхема имеет сложность $O(n \log n)$, когда новый алгоритм имеет среднюю сложность $O(n \log h)$. Очевидно, что такая разница и даёт выигрыш на маленьком $h$.

\begin{figure}[H]
	\centering
	\includesvg{comparison_10000}
	\caption{Сравнение алгоритмов при $n = 10000$}
	\label{img:comparison_10000}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg{comparison_50000}
	\caption{Сравнение алгоритмов при $n = 50000$}
	\label{img:comparison_50000}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg{comparison_100000}
	\caption{Сравнение алгоритмов при $n = 100000$}
	\label{img:comparison_100000}
\end{figure}

Результат сравнения в графиках не является репрезентативным, потому что сравнение по новой методике должно проводиться по двум параметрам - $n$ (количество точек в изначальном множестве) и $h$ (количество точек в выпуклой оболочке). Таблица \ref{table:ratio} показывает такое сравнение. Значениями таблицы является отношение времени работы нового алгоритма к времени работы алгоритма Грэхема переведённое в проценты.

\begin{table}[H]
	\centering
	\caption{Отношение времени работы нового алгоритма к времени работы алгоритма Грэхема}
	\label{table:ratio}
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Количество} & \multicolumn{8}{c|}{Процент точек в выпуклой оболочке} \\ \cline{2-9}
точек& 1 & 2 & 3 & 4 & 5 & 6 & 8 & 10 \\ \hline
1000  &  -17.23 & -9.35  & -3.59  & -7.09  & -2.95 & -2.42 &  8.59 & 7.59  \\ \hline
2500  &  -20.22 & -20.23 & -16.33 & -12.38 & -2.95 & -3.96 &  4.93 & 10.19 \\ \hline
5000  &  -27.5  & -22.44 & -12.4  & -6.21  & -6.06 &  0.39 &  8.18 & 15.19 \\ \hline
7500  &  -26.99 & -17.54 & -12.36 & -5.44  &  1.79 &  2.3  &  9.68 & 19.51 \\ \hline
10000 &  -26.92 & -19.25 & -12.07 & -6.38  &  0.85 &  3.6  & 11.67 & 19.91 \\ \hline
25000 &  -25.36 & -13.3  & -5.51  &  0.24  &  6.69 & 10.42 & 19.58 & 26.35 \\ \hline
50000 &  -23.27 & -14.14 & -4.29  &  1.28  &  5.99 & 11.1  & 17.01 & 23.7  \\ \hline
75000 &  -21.03 & -10.91 & -2.71  &  4.58  &  8.37 & 13.65 & 21.27 & 28.45 \\ \hline
100000&  -18.03 & -7.16  &  0.27  &  5.76  & 12.72 & 15.99 & 25.81 & 37.51 \\ \hline
	\end{tabular}
\end{table}

Заметим, что новый алгоритм довольно быстро сильно превышает время работы алгоритма Грэхема при большом количестве точек в выпуклой оболочке. Это происходит из-за того, что структура данных, которая лежит в основе тестируемого алгоритма - это красно-чёрное дерево. Это сбалансированное бинарное дерево поиска, которое хранится в памяти не последовательно. Это важно, поскольку алгоритм Грэхема использует только массив точек и работает только на нём, когда же наш алгоритм требует размещения точек в дереве. Это плохо работает на современных машинах из-за устройства кэша процессора. Поэтому улучшением работы может быть замена структуры на любое другое красно-чёрное дерево.

Возможность замены лежащей в основе структуры данных является ещё одним очень важным преимуществом предлагаемого алгоритма. Человек, использующий алгоритм может выбрать структуру данных на основе своего тестирования на своих данных. Таким образом алгоритм будет соптимизирован под работу над конкретным набором данных.

\section{Выводы}
