\chapter{Разработка предлагаемого алгоритма и системы сравнения быстродействия алгоритмов построения выпуклых оболочек} \label{chapt3}

\section{Разработка методики сравнения алгоритмов}

\subsection{Минусы классической методики сравнения}

Большинство сравнений алгоритмов построения выпуклой оболочки не зависят от выходных данных. А как было продемонстрировано в предыдущих главах именно от количества точек в финальной выпуклой оболочке зависит время работы многих из них.

Большинство сравнений полагается на некоторые допущения по случайному распределению точек на плоскости. Вот некоторые из них~\cite{chadnov2004algorithmsComparison}:

\begin{enumerate}
	\item Равномерное распределение в единичном квадрате.
	\item Равномерное распределение в единичном круге.
	\item Нормальное распределение в единичном квадрате.
	\item Распределение Лапласа в единичном квадрате с центром распределения в точке $(0.5, 0.5)$.
	\item Равномерное распределение точек на окружности.
\end{enumerate}

Как видно из этого списка, количество точек в финальной выпуклой оболочке будет каким-то фиксированным для каждого из выбранного способа. Например, при распределении внутри единичного круга число вершин в выпуклой оболочке для $n$ точек будет $\theta(n^{1/3})$~\cite{algolist2010convexhull}. А для распределения на окружности очевидно, что количество точек в выпуклой оболочке будет равно изначальному количеству точек.

Такой подход не даёт полноту картины для разного количества точек в выпуклой оболочке. Он привязывает к сравнению всего лишь на основе одного параметра "--- количества точек. Поэтому было решено разработать новую методику сравнения алгоритмов, которая бы давала эту возможность.

\subsection{Идея сравнения на основе выходного параметра алгоритма}

Основная идея, которая будет использоваться при сравнении алгоритмов "--- это мы будем сравнивать не только опираясь на $n$ (изначальное количество точек), но и на $h$ (количество точек в выпуклой оболочке).

Для того, чтобы достичь этого, необходимо придумать способ генерировать тестовые данные с фиксированным процентом точек, которые будут в выпуклой оболочке. Сперва генерация происходит на окружности, эти точки точно будут на выпуклой оболочке. Это показано на рисунке \ref{img:points_gen_1}. После чего необходимо сгенерировать точки, которые будут лежать внутри выпуклой оболочки и не попадут в неё. Это можно сделать с помощью генерации точек внутри круга, что показано на рисунке \ref{img:points_gen_2}. Финальным шагом мы перемешиваем эти точки и всё. Входные данные с фиксированным процентом точек в выпуклой оболочке готовы.

\begin{figure}
	{\centering
		\hfill
		\subbottom[\label{img:points_gen_1}]{%
			\includesvg[width=0.45\linewidth]{gen_1}}
		\hfill
		\subbottom[\label{img:points_gen_2}]{%
			\includesvg[width=0.45\linewidth]{gen_2}}
		\hfill
	}
	\caption{Генерирование точек с фиксированным процентом в выпуклой оболочке}
	\label{img:points_gen}
\end{figure}

Проблема генерации точек внутри круга как на рисунке \ref{img:points_gen_2} в том, что эти точки всё равно могут стать выпуклой оболочкой множества. Этот случай показан на рисунке \ref{img:gen_error}. На этом рисунке красные точки слева и справа были сгенерированы внутри круга, но они попали в выпуклую оболочку.

\begin{figure}
	\centering
	\includesvg[width=0.7\linewidth]{gen_error}
	\caption{Ошибка генерации точек внутри выпуклой оболочки}
	\label{img:gen_error}
\end{figure}

Для того, что исправить эту проблему необходимо после генерации точек, которые будут лежать на выпуклой оболочке генерировать остальные точки внутри получившегося многоугольника. Как же это сделать? Для того, чтобы сгенерировать случайную точку внутри многоугольника необходимо разбить его на треугольники, что показано на рисунке \ref{img:triangles}.

\begin{figure}
	\centering
	\includesvg[width=0.7\linewidth]{triangles}
	\caption{Разбиение выпуклого многоугольника на треугольники}
	\label{img:triangles}
\end{figure}

Заметим, что равномерный случайный выбор треугольника не может быть произведён, потому что треугольники имеют разные площади, поэтому выбирать треугольник, в котором будет генерироваться случайная точка необходимо с весами, равными площадям этих треугольников. После чего остается только вычислить случайную точку $P$ внутри треугольника, который составляют точки $A, B, C$. Это можно сделать с помощью формулы~\eqref{eq:randomInTriangle}~\cite{osada2002shape}.

\begin{equation}\label{eq:randomInTriangle}
P = (1 - \sqrt{r1}) * A + (\sqrt{r1} * (1 - r2)) * B + (\sqrt{r1} * r2) * C
\end{equation}
где $r1, r2$ "--- это равномерно распределённое случайное число в отрезке [0, 1].

\section{Сравнение алгоритмов}

\subsection{Описание используемой библиотеки и параметров сравнения}

Для сравнительного тестирования была выбрана библиотека CGAL, как одна из самых популярных библиотек для вычислительной геометрии. Эта библиотека предоставляет несколько функций для построения выпуклой оболочки~\cite{cgalconvexhull}:
\begin{itemize}
	\item $ch\_akl\_toussaint$ "--- функция, использующая алгоритм Akl-Toussaint~\cite{akl1978fast}, сложность равна $O(n \log n)$;
	\item $ch\_bykat$ "--- функция, использующая алгоритм Eddy~\cite{eddy1977new}, сложность равна $O(nh)$;
	\item $ch\_bykat$ "--- функция, использующая нерекурсивную версию алгоритма Eddy~\cite{bykat1978convex}, сложность равна $O(nh)$;
	\item $ch\_graham\_andrew$ "--- функция, использующая версию Andrew алгоритма Грэхема~\cite{andrew1979another}, сложность равна $O(n \log n)$;
	\item $ch\_jarvis$ "--- функция, использующая алгоритм Джарвиса~\cite{jarvis1973Jarvis}, сложность равна $O(nh)$.
\end{itemize}

Как видно всего 2 алгоритма имеют сложность $O(n \log h)$. Мы будем проводить сравнение с алгоритмом Грэхема как с самым популярным из предложенных.

Программа, которая используется для сравнения алгоритмов лежит в свободном доступе~\cite{matrokhin2017github} и включает в себя 2 модуля:
\begin{enumerate}
	\item Написанный на C++ первый модуль включает в себя запуск и замер времени работы алгоритмов построения выпуклой оболочки. Оболочка строится для множества точек, которые были записаны в файл. Также этот модуль включает имплементацию разработанного в данной работе алгоритма.
	\item Написанный на Python второй модуль включает в себя прежде всего генерацию изначального множества точек, удовлетворяющим условия тестирования, и запуск программы для этих данных. После запуска строятся графики и выводятся табличные данные по полученным результатам.
\end{enumerate}

Для сравнения алгоритмов использовался среднестатистический компьютер. Тем не менее, понятно, что всё это можно повторить и на любых других системах.

Ниже приведены параметры системы, на которой проводилось сравнение:
\begin{enumerate}
	\item Процессор: Intel Core i5-6600K
	\item Оперативная память: Kingston HyperX FURY Black DDR4 2666MHz 8GB
	\item Операционная система: Windows 10 (Version 1803)
	\item Компилятор: Microsoft (R) C/C++ Optimizing Compiler Version 19.11.25547 for x86
\end{enumerate}

\subsection{Результаты классического сравнения}

Как уже говорилось, обычно алгоритмы сравнивают только по одному параметру "--- количеству точек в изначальном множестве. Эти точки обычно распределяют по какому-либо закону на плоскости. Рассмотрим как это сравнение работает при сравнении нашего алгоритма с алгоритмом Грэхема.

Результат сравнения алгоритмов при генерации точек, равномерно распределённых внутри единичного круга, показан на рисунке \ref{img:classic_in_circle}. При двумерном распределении точек показан на рисунке \ref{img:classic_gauss}.

\begin{figure}
	\centering
	\includesvg[width=0.85\linewidth]{classic_in_circle}
	\caption{Сравнение алгоритмов при равномерном распределении внутри круга}
	\label{img:classic_in_circle}
\end{figure}

\begin{figure}
	\centering
	\includesvg[width=0.85\linewidth]{classic_gauss}
	\caption{Сравнение алгоритмов при двумерном нормальном распределении точек}
	\label{img:classic_gauss}
\end{figure}

Смотря на эти графики можно подумать, что предлагаемый алгоритм намного лучше алгоритма Грэхема во всех случаях, но это конечно не так. Почему же так происходит? Всё дело в малом количестве точек, находящихся в выпуклой оболочке при таком распределении. Асимптотика количества точек равна $\theta( n^{1/3} )$ для распределения внутри круга и $\theta{(\log n)^{1/2}}$ для двумерного нормального распределения~\cite{algolist2010convexhull}. Хорошо видно, что на втором графике новый алгоритм ещё больше выигрывает у алгоритма Грэхема из-за меньшего количества точек в выпуклой оболочке.

Рассмотрим, что происходит для равномерного распределения внутри круга. Получается, что сложность алгоритма Грэхема на таком графике равна $O(n \log{n})$, когда сложность нового алгоритма равна $O(n \log{n^{1/3}})$. Для наглядности посмотрим как эти функции выглядят на графике \ref{img:charts_comparison}. Надо понимать, что графики~\ref{img:classic_in_circle} и \ref{img:charts_comparison} не полностью совпадают, так как количество точек в выпуклой оболочке может быть и больше $n^{1/3}$. Это лишь нижняя граница. Также влияет константа, которая заложена в работе алгоритма.

\begin{figure}[hbt]
	\centering
	\includesvg[width=0.9\linewidth]{charts_comparison}
	\caption{Функции сложности алгоритмов при классическом сравнении}
	\label{img:charts_comparison}
\end{figure}

Следующий рисунок \ref{img:classic_on_circle} показывает результат сравнения при генерации точек на окружности. На этом графике можно видеть противоположную ситуацию. Новый алгоритм полностью проигрывает алгоритму Грэхема. Понятно, что тут происходит аналогичная ситуация, только в обратную сторону. Количество точек в выпуклой оболочке приближается к количеству точек в изначальном множестве. Понятно, что при этом сложность обоих алгоритмов будет одинакова, и поэтому будет проигрывать алгоритм с худшей константой. Что мы и видим на графике.

\begin{figure}[hbt]
	\centering
	\includesvg[width=0.9\linewidth]{classic_on_circle}
	\caption{Сравнение алгоритмов при равномерном распределении внутри круга}
	\label{img:classic_on_circle}
\end{figure}

Как видно классическая методика имеет ряд недостатков:
\begin{itemize}
	\item результат кардинально различается от выбранного способа распределения точек, что показано на рисунках \ref{img:classic_in_circle} и \ref{img:classic_on_circle}, это даёт пространство для манипуляции результатами;	
	\item графики не показывают в какой момент нужно менять алгоритм, чаще просто сильно выигрывает один из алгоритмов.
\end{itemize}

Очевидно, что необходимо сравнить алгоритмы с помощью новой методики, чтобы понять какой из них выигрывает в каком случае и когда стоит использовать разработанный в данной работе алгоритм построения выпуклой оболочки.

\subsection{Результаты сравнения с помощью новой методики}

Один из способов протестировать алгоритм с помощью новой методики "--- это зафиксировать количество точек в изначальном множестве и изменять процент точек, которые находятся в выпуклой оболочке. Если сделать это, то получатся графики \ref{img:comparison_5000}, \ref{img:comparison_10000}, \ref{img:comparison_50000} для 10, 50 и 100 тысяч точек соответственно.

\begin{figure}[hbt]
	\centering
	\includesvg{comparison_5000}
	\caption{Сравнение алгоритмов при $n = 5000$}
	\label{img:comparison_5000}
\end{figure}

\begin{figure}
	\centering
	\includesvg{comparison_10000}
	\caption{Сравнение алгоритмов при $n = 10000$}
	\label{img:comparison_10000}
\end{figure}

\begin{figure}
	\centering
	\includesvg{comparison_50000}
	\caption{Сравнение алгоритмов при $n = 50000$}
	\label{img:comparison_50000}
\end{figure}

Легко видеть, что предлагаемый в этой работе алгоритм работает быстрее алгоритма Грэхема только на маленьком проценте точек. Это объясняется как раз лучшей сложностью алгоритма. Алгоритм Грэхема имеет сложность $O(n \log n)$, когда новый алгоритм имеет среднюю сложность $O(n \log h)$. Очевидно, что такая разница и даёт выигрыш на маленьком $h$.

Тем не менее, результат сравнения в графиках не является репрезентативным, потому что сравнение по новой методике должно проводиться по двум параметрам "--- $n$ (количество точек в изначальном множестве) и $h$ (количество точек в выпуклой оболочке). Таблица \ref{table:ratio} показывает такое сравнение. Значениями таблицы является отношение времени работы нового алгоритма к времени работы алгоритма Грэхема переведённое в проценты. Для удобства мы заменили прямое использование $h$ как количества точек на процент точек изначального множества, которые войдут в выпуклую оболочку.

\begin{table}[hbt]
	\centering
	\caption{Отношение времени работы нового алгоритма к времени работы алгоритма Грэхема}
	\label{table:ratio}
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Количество} & \multicolumn{8}{c|}{Процент точек в выпуклой оболочке} \\ \cline{2-9}
точек& 1 & 2 & 3 & 4 & 5 & 6 & 8 & 10 \\ \hline
1000  & -17.03 & -16.88 & -0.08 & -3.99 & 3.14  & 2.94  & 10.69 & 16.2  \\ \hline
2500  & -23.19 & -13.25 & -7.71 & -7.24 & 0.01  & -0.08 & 9.13  & 14.4  \\ \hline
5000  & -19.86 & -14.16 & -12.23& -1.77 & -2.06 & 2.45  & 11.27 & 17.79 \\ \hline
7500  & -23.57 & -15.14 & -8.26 & -1.92 & 1.62  & 6.92  & 8.31  & 17.85 \\ \hline
10000 & -22.14 & -14.15 & -8.25 & -1.32 & 3.82  & 5.81  & 13.49 & 21.39 \\ \hline
25000 & -19.54 & -10.57 & -2.22 & 3.72  & 7.97  & 12.42 & 21.92 & 27.96 \\ \hline
50000 & -20.1  & -10.14 & -3.41 & 4.03  & 8.11  & 12.64 & 19.69 & 26.03 \\ \hline
75000 & -17.74 & -8.11  & -0.11 & 5.25  & 8.95  & 16.64 & 23.07 & 31.15 \\ \hline
100000& -15.61 & -4.2   & 2.59  & 9.45  & 13.48 & 19.6  & 27.81 & 41.18 \\ \hline
	\end{tabular}
\end{table}

Заметим, что новый алгоритм довольно быстро сильно превышает время работы алгоритма Грэхема при большом количестве точек в выпуклой оболочке. Проблема состоит в том, что алгоритм Грэхема проводит все свои операции над лежащими в массиве данными, что позволяет отлично использовать кэш процессора. В это же время структура данных, которая лежит в основе нашего алгоритма "--- это красно-черное дерево. Это сбалансированное двоичное дерево поиска, которое хранит свои данные случайно в памяти. Поэтому при итерации по этой структуре данных мы теряем довольно много времени. Понятно, что можно попробовать улучшить предлагаемый в работе алгоритм заменой структуры данных, лежащей в основе, на любое другое сбалансированное двоичное дерево поиска. Или даже не на дерево, а в целом на структуру, которая имеет необходимые для работы алгоритмы операции.

Как было сказано выше, асимптотика роста количества точек в выпуклой оболочке при равномерном распределении довольно мала. Очевидно, что на больших числах 20 процентов точек в выпуклой оболочке при таких распределениях "--- это почти недостижимая величина. Поэтому необходимо посмотреть как алгоритм работает на маленьком проценте точек при большом количестве точек в изначальном множестве. Именно это показывают графики \ref{img:comparison2_50000} и \ref{img:comparison2_500000} при количестве точек 50000 и 500000 соответственно и таблица \ref{table:small_n_ratio} для разных $n$.

\begin{figure}[hbt]
	\centering
	\includesvg{comparison2_50000}
	\caption{Сравнение алгоритмов при $n = 50000$ на маленьком проценте точек в выпуклой оболочке}
	\label{img:comparison2_50000}
\end{figure}

\begin{figure}[hbt]
	\centering
	\includesvg{comparison2_500000}
	\caption{Сравнение алгоритмов при $n = 500000$ на маленьком проценте точек в выпуклой оболочке}
	\label{img:comparison2_500000}
\end{figure}

\begin{table}[hbt]
	\centering
	\caption{Отношение времени работы нового алгоритма к времени работы алгоритма Грэхема на маленьком проценте точек в выпуклой оболочке}
	\label{table:small_n_ratio}
	\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Количество} & \multicolumn{7}{c|}{Процент точек в выпуклой оболочке} \\ \cline{2-8}
точек& 0.1 & 0.5 & 1 & 1.5 & 2 & 2.5 & 3 \\ \hline
10000 & -45.31 & -30.37 & -21.9 & -16.14 & -15.5 & -11.09 & -5.19 \\ \hline
25000 & -39.21 & -27.47 & -20.36 & -12.28 & -10.42 & -5.31 & -2.4 \\ \hline
50000 & -39.75 & -28.59 & -20.44 & -14.43 & -9.64 & -6.5 & -2.1 \\ \hline
75000 & -39.55 & -28.48 & -17.47 & -12.4 & -7.3 & -1.57 & -0.27 \\ \hline
100000 & -40.46 & -26.8 & -16.16 & -10.13 & -4.48 & -1.72 & 2.69 \\ \hline
250000 & -34.9 & -21.67 & -8.29 & -2.54 & 1.43 & 6.87 & 10.27 \\ \hline
500000 & -31.12 & -13.63 & -2.92 & 5.09 & 14.81 & 25.23 & 32.76 \\ \hline
1000000 & -28.5 & -7.02 & 9.17 & 25.45 & 36.58 & 45.57 & 51.46 \\ \hline
	\end{tabular}
\end{table}

Как видно на маленьких процентах при среднем количестве точек в изначальном множестве разработанный алгоритм работает лучше всего. Это именно те условия, для которых предлагается его использовать. Таким образом посмотрев на эту таблицу можно легко выбрать алгоритм, который нужен для конкретных практических целей.

Возможность замены лежащей в основе структуры данных является ещё одним очень важным преимуществом предлагаемого алгоритма. Человек, использующий алгоритм может выбрать структуру данных на основе своего тестирования на своих данных. Таким образом алгоритм будет оптимизирован под работу над конкретным набором данных.

\section{Выводы}

В этой главе были показаны следующие ключевые моменты:
\begin{enumerate}
	\item Предлагаемый в данной работе алгоритм построения выпуклой оболочки имеет право на существование и может конкурировать с другими алгоритмами на определённом множестве задач.
	\item Разработанный алгоритм может быть усовершенствован и оптимизирован под конкретный класс задач с помощью изменения базовой структуры данных, на которой он работает.
	\item Классическая методика оценки быстродействия алгоритмов работает не так хорошо применительно к задачам построения выпуклой оболочки, так как время работы алгоритмов для конкретно этой задачи сильно зависит от выходных данных.
	\item Была предложена новая методика оценки быстродействия. Она была использована и было показано, что с её помощью удобно сравнивать алгоритмы между собой. Методика сразу показывает сильные и слабые стороны алгоритма, так как проводится анализ в зависимости от двух параметров.
\end{enumerate}

По результатам тестирования можно предложить использовать предлагаемый в данной работе алгоритм в задачах, где заранее известно, что количество точек в выпуклой оболочке не будет превышать некоторого процента от изначального количества. Как было показано выше это довольно частый случай на практике.

Также заметим, что с помощью методики сравнения алгоритмов, необходимо провести отдельное исследование, в котором выяснить насколько хорошо алгоритм работает с другими структурами данных.
